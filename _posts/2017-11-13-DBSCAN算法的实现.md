---
layout: post
title: DBSCAN算法的实现
description: 学习DBSCAN算法的心得体会
tag: 算法  数据挖掘
---

### 聚类算法概述

聚类算法是数据挖掘领域基础的算法之一，它是一种无监督的学习。

假设邀请人们参加一个聚会，不加任何干涉的情况下，一段时间之后人们会自动地分成一个一个的圈子进行交流，这就是聚类思想的一个体现。我们把人的肤色、身高、性别、年龄、学历、兴趣等等要素看做**人**的特性，那么别的人对于此人来说，必然会存在与其最相似的一人，因为物以类聚，所以人会自动分为圈子。

数据中每个数据点都可以获得它与其它数据点之间的距离关系，显然，若数据点A与数据点B的距离小于A与除A、B以外任意数据点时，可以视A、B为具有**相似特征**的两个数据点，我们随机取数点将其视为某几个聚类的中心点，将其它所有点按照距离最小原则划分到这几个中心点代表的聚类中，这样就生成了几个初步的聚类；

以上产生的聚类是由随机点为基础生成的，具有很大的不确定性，而我们需要的是**聚类之间关系不密切而聚类内部之间关系密切**的聚类，因此要进行**迭代**；

在上一步中我们获得了数个聚类，而一般来说，聚类的中心并不是其最优意义上的中心，我们对于每个聚类，按照一定关系计算出其最优意义上的中心，再以此中心为基础，重新进行聚类，可以预料到，迭代到某一步的时候，聚类中心会非常逼近聚类最优的中心，使得**下一次迭代基本甚至完全不发生中心的更换，**至此就能获得最优的聚类结果；

而计算最优意义上的中心方式的不同，就决定了聚类算法的不同：如果我们将聚类中所有点的各维度值取一个均值，以此得到的点作为得到最优意义上的中心，就是**k均值**算法；如果我们将聚类中每个点相对于聚类中其它点的距离计算出来，累加后取与聚类内其他所有点距离之和最小的点作为最优意义上的中心，就是**k中心点算法**。

不过，k均值与k中心点算法除了受到给定的阈值条件的影响外，还**容易受到初始点选取的影响**，初始点选取得不好很容易掉到算法的**局部极小点**中。此外，二者也会受到数据**凹凸性**的影响，两算法对于凸形数据集有很好聚类效果，但对于凹形数据集的效果不是太好。最后，k中心点相比于k均值算法，具有**抗噪声**的特点。

### DBSCAN算法

DBSCAN算法是一种基础的**基于密度**的聚类算法。

我们同样以人作为数据点来分析：上文中提到的聚类思想是以一个人作为中心人物，聚集所有与其特征相似的人作为圈子，然后圈子内部在重新挑选出更能代表圈子特征的人作为新的中心人物，再次聚集圈子，最终获得圈子成员不再变化的数个圈子作为最终结果。显然聚集过程并不只有这一种形式。我们赋予人一个**影响力**的参数，设与其特征相似的人数为其影响力，只有当其影响力达到**一定程度**才能作为**中心人物**获得自己的圈子；我们对每一个人都计算其它所有与其特征相似的人，得出每一个人的影响力，从而得到每一个人代表的圈子（虽然这个人可能并不是中心人物）；可以想象，根据特征相似程度设置的不同，**有的人可能会同时与多个人相似**，处于多个人的影响范围之内，即属于多个圈子，那么这个人像一座**桥梁**，可以实现圈子与圈子之间的沟通、交流、融合；我们以此为基础，选取一个圈子作为基点，根据圈子中的成员，用上述桥梁的思想，**吸收**其它圈子，再**以新的成员为基础继续迭代**，最终就能获得一个大圈子；如果还有圈子未被吸收，我们就继续从未被吸收的圈子中选取一个圈子，重复上述过程，直到所有的圈子都被处理；以上就是基于密度的聚类思想。

基于密度的聚类**不受数据凹凸性的限制**，可以发现**任意形状**的聚类，而且能够**识别噪声点**。

算法的步骤如下：

1、设置聚类半径、聚类最小包含点数等参数，传入数据；

2、对每一数据点，计算它与其它所有数据点的距离，如果不大于聚类半径则认为是可达；

3、对每一数据点，计算与其可达的数据点数，如果不小于聚类最小包含点数，则认为该点是一个聚类中心；

4、随机取一个聚类中心，将所有与其可达的数据点添加到它代表的聚类中，再添加所有与刚才添加的数据点可达的数据点到刚才的聚类中，不断重复直到无法添加；

5、再从未被处理过的聚类中心中随机挑选一个重复上述步骤，直到所有聚类中心都被处理过。

### 代码与解释

代码如下：

	#========================================
	#功能：实现密度聚类算法DBSCAN
	#输入：邻域半径float，核心邻域最小点数int，数据array
	#输出：数据分类array，数据分类个数int
	#========================================
	def DBSCAN(radius, minpts, data):
		if type(3.14) != type(radius):
			print('邻域半径类型必须为float')
			return None
		elif type(0) != type(minpts):
			print('核心邻域最小点数类型必须为int')
			return None
		elif type(array([])) != type(data):
			print('数据类型必须为array')
			return None
		else:
			n = len(data)
			distances = zeros((n, n), dtype='int32')
			for i in range(n):
				for j in range(i, n):
					if sqrt(sum(pow(data[i] - data[j], 2))) <= radius:
						distances[i][j] = 1
						distances[j][i] = 1
			center = []
			cluster_flag = [0 for item in range(n)]
			flag = 0
			processed = set()
			for i in range(n):
				if sum(distances[i], axis=0) >= minpts:
					center.append(i)
			for i in center:
				q = Queue()
				if i not in processed:
					flag = flag + 1
					processed.add(i)
					q.put(i)
					cluster_flag[i] = flag
					while not q.empty():
						sub_i = q.get()
						for j in range(n):
							if distances[sub_i][j] == 1 and j not in processed:
								processed.add(j)
								q.put(j)
								cluster_flag[j] = flag
			return array(cluster_flag), flag

编程语言：python。

算法需要numpy的支持。

else语句以上是参数合法性判断，从else开始：

1、获取data中数据点的个数n，创建了一个n行n列的全零矩阵distances；

2、对每一个数据点计算它与其它数据点的可达性（可达置1），距离选择了欧氏距离，由于第i个点到第j个点的距离等于第j个点到第i个点的距离，因此简化计算步骤，只计算矩阵中的对称部分，将数据替换到全零矩阵中；

3、建立一个空的聚类中心的列表center，一个n长全零的聚类标志列表cluster_flag，一个空的已被处理的点的集合processed，初始化一个0值的聚类标志flag；

4、将矩阵每一行进行累加，第i行累加得到的值就是与第i个数据点可达的数据点的个数，以此为基础判断各个数据点是否为聚类中心点，如果是，则将其添加到center中；

5、对每个处于center内的点，初始化一个队列q，如果该点没有被处理过（即没有处于processed中），将该点添加到processed与q中，更新聚类标志（所有点标志初始为0，因此聚类从1开始计算，最终能够留下未被处理的噪声点），在cluster_flag中置该点的flag；

6、当q不为空时，取出队首的数据点，对于每个与其可达的数据点，如果没有被处理过，将该点加入processed与q中，在cluster_flag中置该点的flag；

7、最终返回array形式的cluster_flag，以及聚的类数flag（我自己写了一个画图函数，需要用到这个参数）。
